{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/saicoumar/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "model_name =\"google/gemma-7b\"\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Login if Necessary\n",
    "hf_token = \"\"\n",
    "with open(\"hugging_face_token.txt\",\"r\") as f:\n",
    "    hf_token = f.readline()\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.11k/1.11k [00:00<00:00, 6.49MB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 16.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:00<00:00, 25.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 555/555 [00:00<00:00, 4.41MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 629/629 [00:00<00:00, 4.19MB/s]\n",
      "model.safetensors.index.json: 100%|██████████| 20.9k/20.9k [00:00<00:00, 98.5MB/s]\n",
      "model-00001-of-00004.safetensors: 100%|██████████| 5.00G/5.00G [03:00<00:00, 27.6MB/s]\n",
      "model-00002-of-00004.safetensors: 100%|██████████| 4.98G/4.98G [02:56<00:00, 28.2MB/s]\n",
      "model-00003-of-00004.safetensors: 100%|██████████| 4.98G/4.98G [03:05<00:00, 26.8MB/s]\n",
      "model-00004-of-00004.safetensors: 100%|██████████| 2.11G/2.11G [01:15<00:00, 28.1MB/s]\n",
      "Downloading shards: 100%|██████████| 4/4 [10:19<00:00, 154.82s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.31s/it]\n",
      "generation_config.json: 100%|██████████| 137/137 [00:00<00:00, 989kB/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      " Write python code to solve the following coding problem that obeys the constraints. Please wrap your code answer using ``` and do not include explanations. Write a function that prints the fibonnaci sequence using dynamic programming <end_of_turn>\n",
      " <start_of_turn>model\n",
      " fibonacci_sequence(n):\n",
      " fibonacci_sequence_helper(n, 0, 1)\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "# Prompt Engineering. Format the prompt such that the LLM will process it and give us a desired output. Selective prompts can decrease computation time. \n",
    "\n",
    "user = \"Write a function that prints the fibonnaci sequence using dynamic programming\"\n",
    "\n",
    "user_processed = f\"Write python code to solve the following coding problem that obeys the constraints. Please wrap your code answer using ``` and do not include explanations. {user}\"\n",
    "prompt = f\"<start_of_turn>user\\n {user_processed} <end_of_turn>\\n <start_of_turn>model\"\n",
    "\n",
    "# Encode prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate output. Model parameters can be found: https://huggingface.co/docs/transformers/en/main_classes/text_generation\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    max_new_tokens = 2000,\n",
    "    top_p=0.9,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "# output = model.generate(**inputs,max_new_tokens = 1000,temperature = 0.1)\n",
    "# Decode and print output\n",
    "output = output[0].to(\"cpu\")\n",
    "print(tokenizer.decode(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
